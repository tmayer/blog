{
  
    
        "post0": {
            "title": "User and Item Based Filtering",
            "content": "Table of Contents . 1&nbsp;&nbsp;User and Item Based Filtering1.1&nbsp;&nbsp;Data | . | 2&nbsp;&nbsp;User-based Filtering2.1&nbsp;&nbsp;Step 1: Identify similar users | 2.2&nbsp;&nbsp;Ranking the critics | 2.3&nbsp;&nbsp;Step 2: Recommending Items | 2.4&nbsp;&nbsp;Item-based Filtering | . | 3&nbsp;&nbsp;User or Item-based Filtering? | . Collaborative Filtering is a method in recommendation systems that searches for similar objects in order to create a ranked list of suggestions. In general, we distinguish between two different approaches: . User-based filtering: we first identify the most similar users to the one for whom we want to give recommendations. | Item-based filtering: we first search for the most similar items to the ones that are most relevant to the user (e.g., most liked by the user) | . We will look at both approaches and discuss their advantages. This blog post is very much inspired by Chapter 2 of Toby Segaran&#39;s classic book &quot;Programming Collective Intelligence&quot; (O&#39;Reilly, 2007). . Data . For our discussion we need some recommendation data. There are several ways of expressing a user&#39;s preference, such as binary distinctions (booked a hotel vs. didn&#39;t book the hotel) or more explicit feedback in the form of graded reviews. . We take the exemplary data below (taken from Segaran&#39;s book) and compute the recommendations using the Pandas library. . critics={&#39;Lisa Rose&#39;: {&#39;Lady in the Water&#39;: 2.5, &#39;Snakes on a Plane&#39;: 3.5, &#39;Just My Luck&#39;: 3.0, &#39;Superman Returns&#39;: 3.5, &#39;You, Me and Dupree&#39;: 2.5, &#39;The Night Listener&#39;: 3.0}, &#39;Gene Seymour&#39;: {&#39;Lady in the Water&#39;: 3.0, &#39;Snakes on a Plane&#39;: 3.5, &#39;Just My Luck&#39;: 1.5, &#39;Superman Returns&#39;: 5.0, &#39;The Night Listener&#39;: 3.0, &#39;You, Me and Dupree&#39;: 3.5}, &#39;Michael Phillips&#39;: {&#39;Lady in the Water&#39;: 2.5, &#39;Snakes on a Plane&#39;: 3.0, &#39;Superman Returns&#39;: 3.5, &#39;The Night Listener&#39;: 4.0}, &#39;Claudia Puig&#39;: {&#39;Snakes on a Plane&#39;: 3.5, &#39;Just My Luck&#39;: 3.0, &#39;The Night Listener&#39;: 4.5, &#39;Superman Returns&#39;: 4.0, &#39;You, Me and Dupree&#39;: 2.5}, &#39;Mick LaSalle&#39;: {&#39;Lady in the Water&#39;: 3.0, &#39;Snakes on a Plane&#39;: 4.0, &#39;Just My Luck&#39;: 2.0, &#39;Superman Returns&#39;: 3.0, &#39;The Night Listener&#39;: 3.0, &#39;You, Me and Dupree&#39;: 2.0}, &#39;Jack Matthews&#39;: {&#39;Lady in the Water&#39;: 3.0, &#39;Snakes on a Plane&#39;: 4.0, &#39;The Night Listener&#39;: 3.0, &#39;Superman Returns&#39;: 5.0, &#39;You, Me and Dupree&#39;: 3.5}, &#39;Toby&#39;: {&#39;Snakes on a Plane&#39;:4.5,&#39;You, Me and Dupree&#39;:1.0,&#39;Superman Returns&#39;:4.0} } . import pandas as pd critics = pd.DataFrame(critics).T . critics . Lady in the Water Snakes on a Plane Just My Luck Superman Returns You, Me and Dupree The Night Listener . Lisa Rose 2.5 | 3.5 | 3.0 | 3.5 | 2.5 | 3.0 | . Gene Seymour 3.0 | 3.5 | 1.5 | 5.0 | 3.5 | 3.0 | . Michael Phillips 2.5 | 3.0 | NaN | 3.5 | NaN | 4.0 | . Claudia Puig NaN | 3.5 | 3.0 | 4.0 | 2.5 | 4.5 | . Mick LaSalle 3.0 | 4.0 | 2.0 | 3.0 | 2.0 | 3.0 | . Jack Matthews 3.0 | 4.0 | NaN | 5.0 | 3.5 | 3.0 | . Toby NaN | 4.5 | NaN | 4.0 | 1.0 | NaN | . The data frame shows our toy data set as an input for recommendations. Usually, the matrix is much bigger and way more sparse. Yet, it illustrates the fact that some users (such as Toby) haven&#39;t rated (and thus might not have seen) all the movies. We would like to get a ranking of the missing movies so that we can recommend the movie which most likely fits to their preferences. . In the collaborative filtering approach (be it user or item based), there are the two main steps to get to this ranking (here illustrated with the user-based approach for our movie toy data set): . Define a similarity between objects (either users or items). This step is different from user and item based recommenders and gives them their respective names. In the user-based approach, the similarity is defined in terms of how the users rated the movies. If users give the same or a very similar rating to the same movie, they are considered more similar than if they give very different ratings. We will see below how we can quantify this to arrive at a similarity matrix between users. | The second step takes the previously defined similarity metric and uses it to compute a weighted average to obtain the final recommendation scores. Once we identified a similarity among users, we will check all the missing movies and weigh the ratings that the other users gave depending on their similarity to the user at hand. | User-based Filtering . Now we want to go through the different steps for user-based filtering and compute the similarities and rankings. . Step 1: Identify similar users . There are multiple ways to define the similarity among users based on the ratings they gave to the movies. However, there is one particular method that a) is easy to compute with Pandas data frames and b) works particularly well when the ratings are not normalized: the Pearson correlation coefficient. . Pearson correlation has values between -1 and +1, where a negative value indicates that the ratings from two users tend to correlate negatively whereas a positive value implies positive correlation. Negative correlation means that a very high rating from the first user would imply a rather low rating from the other while a positive correlation means that both users&#39;s ratings are similar in their magnitude. . In Pandas, there is a very convenient way to get the Pearson correlation coefficients for a data frame by using the corr() method. You just have to decide for which items in our data frame the correlation should be computed, for the users or the movies. In our case, we&#39;re interested in the users, that&#39;s why we have to transpose the data frame first (by using the .T shortcut, i.e., critics.T) before applying the correlation method. . critics.T.corr(method=&#39;pearson&#39;) . Lisa Rose Gene Seymour Michael Phillips Claudia Puig Mick LaSalle Jack Matthews Toby . Lisa Rose 1.000000 | 0.396059 | 0.404520 | 0.566947 | 0.594089 | 0.747018 | 0.991241 | . Gene Seymour 0.396059 | 1.000000 | 0.204598 | 0.314970 | 0.411765 | 0.963796 | 0.381246 | . Michael Phillips 0.404520 | 0.204598 | 1.000000 | 1.000000 | -0.258199 | 0.134840 | -1.000000 | . Claudia Puig 0.566947 | 0.314970 | 1.000000 | 1.000000 | 0.566947 | 0.028571 | 0.893405 | . Mick LaSalle 0.594089 | 0.411765 | -0.258199 | 0.566947 | 1.000000 | 0.211289 | 0.924473 | . Jack Matthews 0.747018 | 0.963796 | 0.134840 | 0.028571 | 0.211289 | 1.000000 | 0.662849 | . Toby 0.991241 | 0.381246 | -1.000000 | 0.893405 | 0.924473 | 0.662849 | 1.000000 | . As can be seen in the correlation matrix, some users are more similar to each other than others. For example, users Toby and Michael Phillips show completely opposite behavior, indicated by a Pearson coefficient of -1. This is because they only share ratings for two movies (&#39;Snakes on a Plane&#39; and &#39;Superman Returns&#39;) and those ratings are completely negatively correlated. Compared to their respective average values (3.25 for Michael Phillips and 4.25 for Toby), they gave opposite directions for these two movies. While Phillips gave a lower rating for &#39;Snakes&#39; (3.0), Toby gave a higher value (4.5) and vice versa for &#39;Superman Returns&#39; (4.0 in Toby&#39;s case and 3.5 in Phillips&#39;). . On the other hand, Toby and Lisa Rose&#39;s ratings are very similar to each other, showing a Pearson value of 0.99. . Users always have a Pearson correlation coefficient of 1 to themselves because, obviously, their ratings show a perfect correlation. The matrix is also symmetrical as the correlation is not dependent on order. . import numpy as np def euclidean(a, b): eucl = np.square(a-b).sum() return 1/(1+eucl) . critics.T.corr(method=euclidean) . Lisa Rose Gene Seymour Michael Phillips Claudia Puig Mick LaSalle Jack Matthews Toby . Lisa Rose 1.000000 | 0.148148 | 0.444444 | 0.285714 | 0.333333 | 0.210526 | 0.222222 | . Gene Seymour 0.148148 | 1.000000 | 0.210526 | 0.133333 | 0.129032 | 0.800000 | 0.108108 | . Michael Phillips 0.444444 | 0.210526 | 1.000000 | 0.571429 | 0.285714 | 0.181818 | 0.285714 | . Claudia Puig 0.285714 | 0.133333 | 0.571429 | 1.000000 | 0.173913 | 0.181818 | 0.235294 | . Mick LaSalle 0.333333 | 0.129032 | 0.285714 | 0.173913 | 1.000000 | 0.137931 | 0.307692 | . Jack Matthews 0.210526 | 0.800000 | 0.181818 | 0.181818 | 0.137931 | 1.000000 | 0.117647 | . Toby 0.222222 | 0.108108 | 0.285714 | 0.235294 | 0.307692 | 0.117647 | 1.000000 | . Ranking the critics . With the Pearson correlation matrix we can compute a ranking of critics for each user. To this end, we define the function top_matches that makes use of the corr() method of data frames for the similary scores. To get a proper similarity ranking, we first identify all users other than the one under investigation and return a Pandas Series that is created by indexing the correlation matrix for the respective user. Finally, the series is sorted in reverse order to return the top similar users. . def top_matches(critics, person, n=10, method=&#39;pearson&#39;): other_people = [p for p in critics.T.columns if p != person] corr_persons = critics.T.corr(method=method) scores = corr_persons.loc[other_people, person] scores.sort_values(ascending=False, inplace=True) return scores.head(n) . As mentioned above, the most similar user to Toby is Lisa Rose, showing a correlation value of almost 1, whereas the least similar is Michael Phillips. . top_matches(critics, &#39;Toby&#39;) . Lisa Rose 0.991241 Mick LaSalle 0.924473 Claudia Puig 0.893405 Jack Matthews 0.662849 Gene Seymour 0.381246 Michael Phillips -1.000000 Name: Toby, dtype: float64 . Step 2: Recommending Items . Having defined the similarity of users, we can move to the second step: how to compute the actual recommendations for user Toby. For this, we first select those movies in our toy dataset that Toby hasn&#39;t ranked yet (assuming he hasn&#39;t watched them either). These missing movies are the basis for our recommendations. All we need to do now is rank them in the appropriate order so that the movie that we assume he would rate highest comes up first in our ranking. . This begs the question how we make the predictions. The basic idea is pretty simple: we take the ratings from other users for the missing movies and compute the weighted average of the rankings for each movie. But where do we take the weights from? It&#39;s very easy. We take advantage of the similarities that we worked out above. Thus, the final score for each movie (for user Toby) is calculated by taking the sum of the rating times the similarity (to Toby) for each movie divided by the overall sum of the similarities. . $ score_{movie} = frac{ sum_{i=1}^{users} sim_{user_i, Toby} * rating_{user_i, movie}}{ sum_{i=1}^{users} sim_{user_i, Toby}}$ . In Pandas, we can use the isnull() method on the Series to identify all the missing movies for the user. Given the similarity rankings from our previously defined function top_matches, we remove those users whose correlation coefficient is negative. The rationale is that we only want to stick to users that rate positively correlated with Toby. We then index into the ratings data frame, extracting only those cells with similar users and missing films. For the actual computation of the scores we need two objects: . the subset of the cells from the original data frame of ratings giving the similar users and the missing movies (cores) | the filtered user similarities where all negatively correlated users to Toby have been discarded (sims) | def get_recommendations(critics, person): missing_films = critics.T[critics.T[person].isnull()].index.values person_similarities = top_matches(critics, person, n=len(critics)) person_similarities = person_similarities[person_similarities &gt; 0] movies_scores = critics.loc[person_similarities.index, missing_films] return movies_scores, person_similarities . scores, sims = get_recommendations(critics, &#39;Toby&#39;) scores . Lady in the Water Just My Luck The Night Listener . Lisa Rose 2.5 | 3.0 | 3.0 | . Mick LaSalle 3.0 | 2.0 | 3.0 | . Claudia Puig NaN | 3.0 | 4.5 | . Jack Matthews 3.0 | NaN | 3.0 | . Gene Seymour 3.0 | 1.5 | 3.0 | . sims . Lisa Rose 0.991241 Mick LaSalle 0.924473 Claudia Puig 0.893405 Jack Matthews 0.662849 Gene Seymour 0.381246 Name: Toby, dtype: float64 . With these two objects we can first compute the numerator of the final score by taking the ratings for the missing movies and multiply them by the respective user&#39;s similarity to Toby. . films_overview = scores.T * sims films_overview . Lisa Rose Mick LaSalle Claudia Puig Jack Matthews Gene Seymour . Lady in the Water 2.478102 | 2.773420 | NaN | 1.988547 | 1.143739 | . Just My Luck 2.973722 | 1.848947 | 2.680215 | NaN | 0.571870 | . The Night Listener 2.973722 | 2.773420 | 4.020323 | 1.988547 | 1.143739 | . After that, we can easily get the sum of these weighted scores for each movie. . films_overview.sum(axis=1) . Lady in the Water 8.383808 Just My Luck 8.074754 The Night Listener 12.899752 dtype: float64 . All that is left now is to compute the denominator. For this, we take the non-null ratings as a mask for the similarities. The reason for this step is to remove individual similarities from the totals if the user didn&#39;t rate the respective movie. Pandas&#39;s weak typing comes in handy at this step. The boolean return values of the notnull() method are turned into integers (0 for False), which nullify the factor for the respective user. At the end, we obtain a similarity total for each movie. . totals = (scores.notnull().T * sims).sum(axis=1) totals . Lady in the Water 2.959810 Just My Luck 3.190366 The Night Listener 3.853215 dtype: float64 . Finally, we can compute the actual predicted movie ratings for user Toby by taking the sums of the weighted scores and dividing them by the totals. In our toy example, the movie &#39;The Night Listener&#39; gets the highest rating with 3.35 and would thus be our first choice for a recommendation to Toby. . recs = films_overview.sum(axis=1) / totals recs.sort_values() . Just My Luck 2.530981 Lady in the Water 2.832550 The Night Listener 3.347790 dtype: float64 . Item-based Filtering . So far, we&#39;ve looked at the user similarities first to reduce the search space for our recommendations. However, this comes with a downside when your matrix of ratings includes a large number of users and shows a lot of sparsity, thus making it hard to compute similarities among users when there is little to no overlap. . An alternative approach, called item-based filtering, tries to remedy these problems by dispensing with the user similarity step and searching for similar items right away (hence the name). With our toy dataframe this is very easy as we only need to apply the corr() method again, this time on the items and not on the users: . critics.corr(method=euclidean) . Lady in the Water Snakes on a Plane Just My Luck Superman Returns You, Me and Dupree The Night Listener . Lady in the Water 1.000000 | 0.222222 | 0.222222 | 0.090909 | 0.400000 | 0.285714 | . Snakes on a Plane 0.222222 | 1.000000 | 0.105263 | 0.166667 | 0.051282 | 0.181818 | . Just My Luck 0.222222 | 0.105263 | 1.000000 | 0.064516 | 0.181818 | 0.153846 | . Superman Returns 0.090909 | 0.166667 | 0.064516 | 1.000000 | 0.053333 | 0.102564 | . You, Me and Dupree 0.400000 | 0.051282 | 0.181818 | 0.053333 | 1.000000 | 0.148148 | . The Night Listener 0.285714 | 0.181818 | 0.153846 | 0.102564 | 0.148148 | 1.000000 | . Again, the same principles apply that we mentioned above for the user similarities. A score close to 1 indicates that the movies are very similar. For example, the movies &#39;Snakes on a Plane&#39; and &#39;Lady in the Water&#39; seem to be alike based on the users&#39;s ratings whereas &#39;Just my Luck&#39; and &#39;Lady in the Water&#39; are quite dissimilar. . To compute the final recommendation scores all we need to do is to identify the missing movies of a given user (Toby) and compute a weighted average of their similarity to all movies the user has already rated together with their similarity to the missing movies. . def get_item_recommendations(critics, person, method=&#39;pearson&#39;): missing_films = critics.T[critics.T[person].isnull()].index.values movie_similarities = critics.corr(method=method) movie_similarities = movie_similarities.loc[~movie_similarities.index.isin(missing_films), missing_films] person_ratings = critics.loc[person, ~critics.columns.isin(missing_films)] return movie_similarities, person_ratings . ms, pr = get_item_recommendations(critics, &#39;Toby&#39;, method=euclidean) ms . Lady in the Water Just My Luck The Night Listener . Snakes on a Plane 0.222222 | 0.105263 | 0.181818 | . Superman Returns 0.090909 | 0.064516 | 0.102564 | . You, Me and Dupree 0.400000 | 0.181818 | 0.148148 | . pr . Snakes on a Plane 4.5 Superman Returns 4.0 You, Me and Dupree 1.0 Name: Toby, dtype: float64 . Like above, we now take the missing movies and weigh them with the similarities that we defined above. The result is an item-based recommendation score that indicates which movies we should recommend next to the user (in this case Toby). . (ms.T * pr).sum(axis=1) . Lady in the Water 1.763636 Just My Luck 0.913567 The Night Listener 1.376586 dtype: float64 . item_recos = (ms.T*pr).sum(axis=1) / ms.sum() item_recos . Lady in the Water 2.473088 Just My Luck 2.598332 The Night Listener 3.182635 dtype: float64 . User or Item-based Filtering? . As shown above, user and item-based filtering mainly differ in the first step: we either have to compute similarities for users or times first. With a very large database, item-based filtering is to be preferred as it is significantly faster to compare the items for users to identify the similarities than to compare a given user to all other users .",
            "url": "https://tmayer.github.io/blog/machinelearning/2020/06/21/user-item-based-filtering.html",
            "relUrl": "/machinelearning/2020/06/21/user-item-based-filtering.html",
            "date": " • Jun 21, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Softmax Explained",
            "content": "Problem . Given a bunch of numbers each representing a value for a given item you want to transform them into a metric to identify the highest value with the following properties: . the resulting metric should normalize all values, that is the sum of all values should be 1 | the metric should favor only one item among the numbers (the one with the highest original value), thus boosting it to make it stand apart more clearly | . Use case . Activation function in the last layer of a classification network. Each item stands for a certain class and only one class is to be selected. In the previous layer any kind of activation can result (normally between 0 and infinity) but we want to have a normalized output at the end that tells us which class has been activated. . Two-step approach . Make all values positive (and boost higher values) | Normalize all values so that they sum to 1. | import numpy as np original_values = np.random.randn(10) . original_values . array([ 0.46836232, -0.7609481 , 1.13598762, 0.55041867, 0.71270642, -0.99529955, 1.3241378 , 1.93287689, 0.43763017, 0.59186142]) . Step 1: Make everything positive while keeping the order of elements constant (monotonicity) . There are various ways of doing it, but a very convenient one is to use each value as the power of exp . step1 = np.exp(original_values) . step1 . array([1.59737606, 0.46722324, 3.1142477 , 1.73397883, 2.03950355, 0.36961271, 3.75894301, 6.90935916, 1.54903192, 1.80734952]) . # check if the ranks in both arrays are still the same (order is preserved) from numpy.testing import assert_array_equal assert_array_equal(np.argsort(step1), np.argsort(step1)) # check if all values are positive assert all(step1 &gt; 0) . Step 2: Normalize the values to lie between 0 and 1. The sum of all values should be 1 . step2 = step1/step1.sum() . step2 . array([0.06842 , 0.02001245, 0.13339177, 0.07427107, 0.08735753, 0.01583153, 0.16100584, 0.2959468 , 0.06634929, 0.07741374]) . # check if all values are between 0 and 1 softmax_values = step2 assert all(0 &lt;= softmax_values) assert all(softmax_values &lt;= 1) # check if the values sum up to 1 from numpy.testing import assert_almost_equal assert_almost_equal(softmax_values.sum(), 1) . # Plot the original_values versus the softmax values. # We sort both arrays in increasing order. You can see that the # line for the softmax_values is slightly steeper, # thus indicating the boost of higher values. import matplotlib.pyplot as plt fig, (ax1, ax2) = plt.subplots(2, 1) ax1.plot(sorted(original_values)); ax1.set(ylabel=&quot;Original values&quot;); ax2.plot(sorted(softmax_values)); ax2.set(ylabel=&quot;Softmax values&quot;); . Usage in Deep Learning frameworks . All Deep Learning frameworks have softmax functions. Here we show the Keras and PyTorch versions. . import keras from keras import backend as K keras_result = keras.activations.softmax( K.variable(value=original_values.reshape(1, -1)), axis=-1).numpy().flatten() . # Since Keras&#39;s softmax function uses a different approach, # the precision of the results varies from numpy.testing import assert_array_almost_equal assert_array_almost_equal(keras_result, softmax_values) . import torch pytorch_result = torch.nn.functional.softmax( torch.tensor(original_values.reshape(1, -1)), dim=1).numpy().flatten() . pytorch_result . array([0.06842 , 0.02001245, 0.13339177, 0.07427107, 0.08735753, 0.01583153, 0.16100584, 0.2959468 , 0.06634929, 0.07741374]) . assert_array_almost_equal(pytorch_result, softmax_values) . Caveat . Since softmax boosts the item with the highest value (winner takes it all), you shouldn&#39;t be using softmax whenever you want to have more than one element in the output (e.g., in multi-label classification scenarios). .",
            "url": "https://tmayer.github.io/blog/deeplearning/2020/05/02/softmax-explained.html",
            "relUrl": "/deeplearning/2020/05/02/softmax-explained.html",
            "date": " • May 2, 2020"
        }
        
    
  
    
  
    
  
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://tmayer.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://tmayer.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}