{
  
    
        "post0": {
            "title": "Softmax Explained",
            "content": "Problem . Given a bunch of numbers each representing a value for a given item you want to transform them into a metric to identify the highest value with the following properties: . the resulting metric should normalize all values, that is the sum of all values should be 1 | the metric should favor only one item among the numbers (the one with the highest original value), thus boosting it to make it stand apart more clearly | . Use case . Activation function in the last layer of a classification network. Each item stands for a certain class and only one class is to be selected. In the previous layer any kind of activation can result (normally between 0 and infinity) but we want to have a normalized output at the end that tells us which class has been activated. . Two-step approach . Make all values positive (and boost higher values) | Normalize all values so that they sum to 1. | import numpy as np original_values = np.random.randn(10) . original_values . array([ 0.46836232, -0.7609481 , 1.13598762, 0.55041867, 0.71270642, -0.99529955, 1.3241378 , 1.93287689, 0.43763017, 0.59186142]) . Step 1: Make everything positive while keeping the order of elements constant (monotonicity) . There are various ways of doing it, but a very convenient one is to use each value as the power of exp . step1 = np.exp(original_values) . step1 . array([1.59737606, 0.46722324, 3.1142477 , 1.73397883, 2.03950355, 0.36961271, 3.75894301, 6.90935916, 1.54903192, 1.80734952]) . # check if the ranks in both arrays are still the same (order is preserved) from numpy.testing import assert_array_equal assert_array_equal(np.argsort(step1), np.argsort(step1)) # check if all values are positive assert all(step1 &gt; 0) . Step 2: Normalize the values to lie between 0 and 1. The sum of all values should be 1 . step2 = step1/step1.sum() . step2 . array([0.06842 , 0.02001245, 0.13339177, 0.07427107, 0.08735753, 0.01583153, 0.16100584, 0.2959468 , 0.06634929, 0.07741374]) . # check if all values are between 0 and 1 softmax_values = step2 assert all(0 &lt;= softmax_values) assert all(softmax_values &lt;= 1) # check if the values sum up to 1 from numpy.testing import assert_almost_equal assert_almost_equal(softmax_values.sum(), 1) . # Plot the original_values versus the softmax values. # We sort both arrays in increasing order. You can see that the # line for the softmax_values is slightly steeper, # thus indicating the boost of higher values. import matplotlib.pyplot as plt fig, (ax1, ax2) = plt.subplots(2, 1) ax1.plot(sorted(original_values)); ax1.set(ylabel=&quot;Original values&quot;); ax2.plot(sorted(softmax_values)); ax2.set(ylabel=&quot;Softmax values&quot;); . Usage in Deep Learning frameworks . All Deep Learning frameworks have softmax functions. Here we show the Keras and PyTorch versions. . import keras from keras import backend as K keras_result = keras.activations.softmax( K.variable(value=original_values.reshape(1, -1)), axis=-1).numpy().flatten() . # Since Keras&#39;s softmax function uses a different approach, # the precision of the results varies from numpy.testing import assert_array_almost_equal assert_array_almost_equal(keras_result, softmax_values) . import torch pytorch_result = torch.nn.functional.softmax( torch.tensor(original_values.reshape(1, -1)), dim=1).numpy().flatten() . pytorch_result . array([0.06842 , 0.02001245, 0.13339177, 0.07427107, 0.08735753, 0.01583153, 0.16100584, 0.2959468 , 0.06634929, 0.07741374]) . assert_array_almost_equal(pytorch_result, softmax_values) . Caveat . Since softmax boosts the item with the highest value (winner takes it all), you shouldn&#39;t be using softmax whenever you want to have more than one element in the output (e.g., in multi-label classification scenarios). .",
            "url": "https://tmayer.github.io/blog/deeplearning/2020/05/02/softmax-explained.html",
            "relUrl": "/deeplearning/2020/05/02/softmax-explained.html",
            "date": " • May 2, 2020"
        }
        
    
  
    
  
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://tmayer.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://tmayer.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}